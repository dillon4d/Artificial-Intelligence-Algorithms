{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sentence_transformers\n",
    "#!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download sentiment datasets (Stanford Sentiment Treebank v2, train and dev).\n",
    "#!gdown \"https://drive.google.com/uc?id=1j6GNle4N6dIZ06r8yHFqW2FEMmKrxVbW\"\n",
    "#!gdown \"https://drive.google.com/uc?id=1WDFTmif6S0boBQf4qR0jy2M_QaywO9QO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 405M/405M [07:59<00:00, 845kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading sentences.\n",
      "Embedding sentences.\n",
      "Reading sentences.\n",
      "Embedding sentences.\n"
     ]
    }
   ],
   "source": [
    "# Setup.\n",
    "import codecs\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load sentence embeddings model.\n",
    "# See: https://www.aclweb.org/anthology/D19-1410.pdf\n",
    "embed_model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\n",
    "# Function that gets the embedding from a sentence.\n",
    "def embedding_fn(sentence):\n",
    "    return embed_model.encode([sentence])[0]\n",
    "\n",
    "# Read datasets.\n",
    "def read_dataset(filepath, n=100):\n",
    "    print(\"Reading sentences.\")\n",
    "    infile = codecs.open(filepath, 'rb', encoding='utf-8')\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    embeddings = []\n",
    "    line_count = 0\n",
    "    for line in infile:\n",
    "        line_count += 1\n",
    "        if line_count == 1:\n",
    "            continue # Skip header\n",
    "        split = line.split(\"\\t\")\n",
    "        sentences.append(split[0])\n",
    "        labels.append(int(split[1])) # Note: 1 is positive, 0 is negative.\n",
    "    print(\"Embedding sentences.\")\n",
    "    embeddings = embed_model.encode(sentences[0:n])\n",
    "    labels = np.stack(labels[0:n])\n",
    "    return embeddings, labels\n",
    "  \n",
    "X_train, Y_train = read_dataset(\"train.tsv\", n=500)\n",
    "X_dev, Y_dev = read_dataset(\"dev.tsv\", n=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 768)\n"
     ]
    }
   ],
   "source": [
    "#use cosine similarity due to high-dimensionality, euclidean does not work as well\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev accuracy: 0.92\n",
      "1\n",
      "Sentence: I like the food but I dislike the atmosphere\n",
      "Label: negative\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def knn(X_train, Y_train, x_dev, k=10):\n",
    "    similarities = cosine_similarity([x_dev], X_train)\n",
    "    sorted_indices = np.argsort(similarities[0], axis=-1)\n",
    "    sorted_indices = sorted_indices[-k:]\n",
    "    near_labels = Y_train[sorted_indices]\n",
    "    num_positive = np.sum(near_labels)\n",
    "    proportion_positive = num_positive/k\n",
    "    print(num_positive)\n",
    "    return 1 if proportion_positive > 0.5 else 0\n",
    "\n",
    "def knn_batch(X_train, Y_train, X_dev, k=10):\n",
    "    # Shape: n_dev x n_train (50,500)\n",
    "    similarities = cosine_similarity(X_dev, X_train)\n",
    "    # Shape: n_dev x n_train (50,500)\n",
    "    sorted_indices = np.argsort(similarities, axis=-1)\n",
    "    sorted_indices = sorted_indices[:,-k:]\n",
    "    near_labels = Y_train[sorted_indices]\n",
    "    num_positive = np.sum(near_labels, axis=-1)\n",
    "    proportion_positive = num_positive/k\n",
    "    predictions = proportion_positive > 0.5 \n",
    "    return predictions\n",
    "  \n",
    "\n",
    "# Get dev accuracy:\n",
    "dev_predictions = knn_batch(X_train, Y_train, X_dev, k=10)\n",
    "dev_accuracy = np.average(dev_predictions == Y_dev)\n",
    "print(\"Dev accuracy: {}\".format(dev_accuracy))\n",
    "\n",
    "# Test your own sentence:\n",
    "sentence = \"I like the food but I dislike the atmosphere\"\n",
    "prediction = knn(X_train, Y_train, embedding_fn(sentence), k=10)\n",
    "print(\"Sentence: {}\".format(sentence))\n",
    "print(\"Label: {}\".format(\"positive\" if prediction else \"negative\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
